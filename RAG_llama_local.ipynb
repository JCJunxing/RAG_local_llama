{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß†üí° Retrieval-Augmented Generation (RAG), enhancing LLM for specific use (mining document from Canadian Ontario government)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 (Base) Pre-set up üõ†Ô∏è\n",
    "#### 1.1 Set up environment üåø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda create --name RAG_LLAMA python=3.11.9    #python version can't be or highter than 3.12\n",
    "#conda remove -n ENV_NAME --all\n",
    "#conda activate RAG_LLAMA\n",
    "#conda install -n RAG_LLAMA ipykernel --update-deps --force-reinstall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Install libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install sentence-transformers\n",
    "!pip install Huggingface_hub\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install langchain\n",
    "!pip install langchain-community\n",
    "!pip install chromadb\n",
    "!pip install ipywidgets\n",
    "!pip install PyPDF2 PyCryptodome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Check GPU and Torch üñ•Ô∏èüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cu118'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load LLM ü§ñüì•\n",
    "\n",
    "We are using open-source `llama-3-8B model`, running locally. If you wish to use a larger model like `GPT-4`, you can adjust the code accordingly. Remember to securely handle and protect your API key.\n",
    "\n",
    "#### 3.1 Login to Hugging Face Hub ü§óüîë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Import `llama-3-8B` ü¶ôüì•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff17c4fa868749949dda295efa46cec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "LLM_model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "LLM_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_model_id, add_bos_token=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Test the model & ouput üß™üñ®Ô∏è\n",
    "\n",
    "\n",
    "We can test if the base LLM is effective and knowledgeable in niche areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "C:\\Users\\yxzhh\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yxzhh\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the date that Ontario has made changes to the Mining Act and created a new regulation, Ontario Regulation 35/24 which replaced Ontario Regulation 240/00? How does this regulation affect the mining industry?\n",
      "Ontario Regulation 35/24 was made on January 1, 2004 and came into effect on January 1, 2005. This regulation replaced Ontario Regulation 240/00. This regulation affects the mining industry by giving the Ministry of Natural Resources and the Ministry of Northern Development and Mines the power to require mining companies to prepare and submit a plan of rehabilitation for each mining operation.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's the date that Ontario has made changes to the Mining Act and created a new regulation, Ontario Regulation 35/24 which replaced Ontario Regulation 240/00?\"\n",
    "inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "\n",
    "outputs = LLM_model.generate(**inputs, max_length=1024)\n",
    "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True, num_return_sequences=1,do_sample=False)\n",
    "\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base LLM might not be aware of news in specific fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Prepare the file & embeddings for Retrieval üìÑüîç\n",
    "\n",
    "#### 4.1 Initialize embedding function üßÆüî¢\n",
    "Begin by setting up the embedding function to transform text data into numerical vectors for efficient retrieval and analysis. Here we use open-source `all-MinLM-L6-v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Create Chromadb collection üóÉÔ∏èüíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient()\n",
    "collection = client.create_collection(\"history_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Read files üìÇüìñ\n",
    "\n",
    "Here, we downloaded several PDF files about on Ontario's mining industry regulations and development from the official government website. We put the files in the \"documents\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_documents(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if filename.endswith('.pdf'):\n",
    "            documents.append(read_pdf(file_path))\n",
    "        elif filename.endswith('.txt'):\n",
    "            documents.append(read_txt(file_path))\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Read documents from a directory\n",
    "directory = \"documents\"   # Change to the folder name where you save the files\n",
    "documents = read_documents(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Chunk text ‚úÇÔ∏èüìÑ\n",
    "\n",
    "We are using `LangChain` with `RecursiveCharacterTextSplitter` to effectively split text into manageable chunks. \n",
    "\n",
    "Change the `chunk_size` considering the amount you want to use as input in LLM.\n",
    "\n",
    "This combination allows for precise segmentation of text based on characters, ensuring each chunk maintains coherence and context throughout the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_documents(documents, chunk_size=512, chunk_overlap=48):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        chunks.extend(text_splitter.split_text(doc))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Chunk the documents\n",
    "chunks = chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Embedding the chunks and add to the collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk 1590\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(chunks)):\n",
    "    chunk = chunks[i]\n",
    "    embedding = embed_model.encode(chunk).tolist()\n",
    "    collection.add(\n",
    "        documents=[chunk],\n",
    "        embeddings=[embedding],\n",
    "        ids=[f\"chunk_{i}\"]\n",
    "    )\n",
    "print(f\"Added chunk {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Retrieve-Augmented-Generate üîçüß†üí°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Retrieve  üîé\n",
    "\n",
    "To query the chunks in the ChromaDB collection and find the top 5 texts most related to a question, the ranking process typically involves using similarity scores derived from these embeddings.\n",
    "\n",
    "The total prompt in LLM will be `5 * chunk_size + Question + Instructions`.\n",
    "\n",
    "Here, the ChromaDB collection has a `query` function to select the top 5 chunks with the highest similarity scores as they are deemed most closely related to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_k=5):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results['documents'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieved information is incorporated into the prompt as inputs for the Large Language Model (LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(query, context):\n",
    "    prompt = f\" Instructions: Answer the above question and stop. Do not generate additional text or questions \\n\\n\\n Respond considering the following context for reference if related:\\n\\n {context} \\n\\n\\n  Question: {query} \\n\\n\\n  Answer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = LLM_model.generate(**inputs, max_length=1024)\n",
    "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True, num_return_sequences=1,do_sample=False)\n",
    "    # Custom stopping logic\n",
    "    stop_sequence = \"Question:\"\n",
    "    response_text = response_text.split(stop_sequence)[1].strip()\n",
    "    return response_text\n",
    "\n",
    "def rag_answer(query):\n",
    "    relevant_texts = retrieve(query)\n",
    "    context = \" \".join(relevant_texts)\n",
    "    answer = generate(query, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can explore the augmented generation capabilities of the LLM. üöÄüí¨\n",
    "\n",
    "Change the `max_lenth` smaller if your systems takes too much time for LLM to inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "C:\\Users\\yxzhh\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the date that Ontario has made changes to the Mining Act and created a new regulation, Ontario Regulation 35/24 which replaced Ontario Regulation 240/00? \n",
      "\n",
      "\n",
      "  Answer: April 1, 2024\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"What's the date that Ontario has made changes to the Mining Act and created a new regulation, Ontario Regulation 35/24 which replaced Ontario Regulation 240/00?\"\n",
    "answer = rag_answer(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the RAG worked!\n",
    "\n",
    "----\n",
    "\n",
    "You can tailor the prompt style as per your requirements.  üé®‚úèÔ∏è\n",
    "\n",
    "After generating output, format it and use it as input for the LLM again to evaluate. üîÑüßê\n",
    "\n",
    "Alternatively, assess the relevance of retrieved content to decide on the need for further online search. üåêüîç\n",
    "\n",
    "This method offers clues for topics the **LLM agent** can explore, with our introduced `LangChain` being particularly useful for such tasks. üïµÔ∏è‚Äç‚ôÇÔ∏èüîó\n",
    "\n",
    "üëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëè\n",
    "----\n",
    "We welcome contributions to this project! Feel free to fork the repository and submit a pull request. ü§ù‚ú®\n",
    "\n",
    "Feel free to reach out if you have any questions or suggestions. Let's make this project even more awesome together! üöÄüòä\n",
    "\n",
    "Happy coding! üíªüéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
