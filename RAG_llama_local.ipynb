{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß†üí° Retrieval-Augmented Generation (RAG), enhancing LLM for specific use (mining document from Canadian Ontario government)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 (Base) Pre-set up üõ†Ô∏è\n",
    "#### 1.1 Set up environment üåø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda create --name RAG_LLAMA python=3.11.9    #python version can't be or highter than 3.12\n",
    "#conda remove -n ENV_NAME --all\n",
    "#conda activate RAG_LLAMA\n",
    "#conda install -n RAG_LLAMA ipykernel --update-deps --force-reinstall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Install libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install sentence-transformers\n",
    "!pip install Huggingface_hub\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install langchain\n",
    "!pip install langchain-community\n",
    "!pip install chromadb\n",
    "!pip install ipywidgets\n",
    "!pip install PyPDF2 PyCryptodome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Check GPU and Torch üñ•Ô∏èüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load LLM ü§ñüì•\n",
    "\n",
    "We are using open-source `llama-3-8B model`, running locally. If you wish to use a larger model like `GPT-4`, you can adjust the code accordingly. Remember to securely handle and protect your API key.\n",
    "\n",
    "#### 3.1 Login to Hugging Face Hub ü§óüîë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Import `llama-3-8B` ü¶ôüì•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a354d48b7b44e5a2a6cdb8faa9bef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "LLM_model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "LLM_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_model_id, add_bos_token=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Test the model & ouput üß™üñ®Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "C:\\Users\\yxzhh\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yxzhh\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's the date that Ontario has made changes to the Mining Act and created a new regulation, Ontario Regulation 35/24 which replaced Ontario Regulation 240/00? Is it the date of publication in the Ontario Gazette or the date of coming into force? If it is the date of publication, then how can I find out the date of publication? Thank you.\n",
      "The date of publication is the date that the regulation was published in the Ontario Gazette. The date of coming into force is the date that the regulation came into force. For the Mining Act, the date of publication is the same as the date of coming into force. The date of publication is usually found on the first page of the regulation. The date of coming into force is usually found on the last page of the regulation.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's the date that Ontario has made changes to the Mining Act and created a new regulation, Ontario Regulation 35/24 which replaced Ontario Regulation 240/00?\"\n",
    "inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "\n",
    "outputs = LLM_model.generate(**inputs, max_length=1024)\n",
    "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True, num_return_sequences=1,do_sample=False)\n",
    "\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Prepare the file & embeddings for Retrieval üìÑüîç\n",
    "\n",
    "#### 4.1 Initialize embedding function üßÆüî¢\n",
    "Begin by setting up the embedding function to transform text data into numerical vectors for efficient retrieval and analysis. Here we use open-source `all-MinLM-L6-v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Create Chromadb collection üóÉÔ∏èüíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient()\n",
    "collection = client.create_collection(\"history_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Read files üìÇüìñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_documents(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if filename.endswith('.pdf'):\n",
    "            documents.append(read_pdf(file_path))\n",
    "        elif filename.endswith('.txt'):\n",
    "            documents.append(read_txt(file_path))\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Read documents from a directory\n",
    "directory = \"documents\"\n",
    "documents = read_documents(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Chunk text ‚úÇÔ∏èüìÑ\n",
    "\n",
    "We are using `LangChain` with `RecursiveCharacterTextSplitter` to effectively split text into manageable chunks. \n",
    "\n",
    "This combination allows for precise segmentation of text based on characters, ensuring each chunk maintains coherence and context throughout the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_documents(documents, chunk_size=256, chunk_overlap=24):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        chunks.extend(text_splitter.split_text(doc))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Chunk the documents\n",
    "chunks = chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Embedding the chunks and add to the collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(chunks)):\n",
    "    chunk = chunks[i]\n",
    "    embedding = embed_model.encode(chunk).tolist()\n",
    "    collection.add(\n",
    "        documents=[chunk],\n",
    "        embeddings=[embedding],\n",
    "        ids=[f\"chunk_{i}\"]\n",
    "    )\n",
    "print(f\"Added chunk {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Retrieve-Augmented-Generate üîçüß†üí°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Retrieve  üîé\n",
    "\n",
    "To query the chunks in the ChromaDB collection and find the top 5 texts most related to a question, the ranking process typically involves using similarity scores derived from these embeddings.\n",
    "\n",
    "Here, the ChromaDB collection has a `query` function to select the top 5 chunks with the highest similarity scores as they are deemed most closely related to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_k=5):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results['documents'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieved information is incorporated into the prompt as inputs for the Large Language Model (LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(query, context):\n",
    "    prompt = f\"Respond considering the following context for reference if related:\\n\\n {context} \\n\\n\\n Instructions: Answer the above question and stop. Do not generate additional text or questions. \\n\\n\\n Question: {query} \\n\\n\\n  Answer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = LLM_model.generate(**inputs, max_length=1024)\n",
    "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True, num_return_sequences=1,do_sample=False)\n",
    "    # Custom stopping logic\n",
    "    #stop_sequence = \"Question:\"\n",
    "    #response_text = response_text.split(stop_sequence)[1].strip()\n",
    "    return response_text\n",
    "\n",
    "def rag_answer(query):\n",
    "    relevant_texts = retrieve(query)\n",
    "    context = \" \".join(relevant_texts)\n",
    "    answer = generate(query, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can explore the augmented generation capabilities of the LLM. üöÄüí¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = \"What's the date that Ontario has made changes to the Mining Act and created a new regulation, Ontario Regulation 35/24 which replaced Ontario Regulation 240/00?\"\n",
    "answer = rag_answer(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tailor the prompt style as per your requirements.  üé®‚úèÔ∏è\n",
    "\n",
    "After generating output, format it and use it as input for the LLM again to evaluate. üîÑüßê\n",
    "\n",
    "Alternatively, assess the relevance of retrieved content to decide on the need for further online search. üåêüîç\n",
    "\n",
    "This method offers clues for topics the **LLM agent** can explore, with our introduced `LangChain` being particularly useful for such tasks. üïµÔ∏è‚Äç‚ôÇÔ∏èüîó\n",
    "\n",
    "üëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëèüëè\n",
    "----\n",
    "We welcome contributions to this project! Feel free to fork the repository and submit a pull request. ü§ù‚ú®\n",
    "\n",
    "Feel free to reach out if you have any questions or suggestions. Let's make this project even more awesome together! üöÄüòä\n",
    "\n",
    "Happy coding! üíªüéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
